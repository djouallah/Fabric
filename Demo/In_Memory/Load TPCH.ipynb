{"cells":[{"cell_type":"code","source":["!pip install duckdb --pre --upgrade"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["sf = 10"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false}},{"cell_type":"code","source":["%%time\r\n","\r\n","import duckdb\r\n","import pathlib\r\n","for x in range(0, sf) :\r\n","  con=duckdb.connect()\r\n","  con.sql('PRAGMA disable_progress_bar;SET preserve_insertion_order=false')\r\n","  con.sql(f\"CALL dbgen(sf={sf} , children ={sf}, step = {x})\") \r\n","  for tbl in ['nation','region','customer','supplier','lineitem','orders','partsupp','part'] :\r\n","     pathlib.Path(f'/lakehouse/default/Files/{sf}/{tbl}').mkdir(parents=True, exist_ok=True) \r\n","     con.sql(f\"COPY (SELECT * FROM {tbl}) TO '/lakehouse/default/Files/{sf}/{tbl}/{x:03d}.parquet' \")\r\n","  con.close()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"0b346e6f-8846-4439-90cb-bdbb2139ac01","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-26T05:03:02.2856867Z","session_start_time":null,"execution_start_time":"2023-06-26T05:05:52.1253355Z","execution_finish_time":"2023-06-26T05:08:41.5294537Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"UNKNOWN":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"79cfa92c-5f34-4de3-85c4-068387bff0b7"},"text/plain":"StatementMeta(, 0b346e6f-8846-4439-90cb-bdbb2139ac01, 5, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 5min 56s, sys: 28.8 s, total: 6min 24s\nWall time: 2min 48s\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","def loadFullDataFromSource(table_name):\n","    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n","    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","full_tables = [\n","    'customer',\n","    'lineitem',\n","    'nation',\n","    'orders' ,\n","    'region',\n","    'partsupp',\n","    'supplier' ,\n","    'part'\n","    ]\n","\n","for table in full_tables:\n","    loadFullDataFromSource(table)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"0b346e6f-8846-4439-90cb-bdbb2139ac01","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-26T05:03:02.2866221Z","session_start_time":null,"execution_start_time":"2023-06-26T05:08:42.6569366Z","execution_finish_time":"2023-06-26T05:10:25.7520547Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"UNKNOWN":0,"SUCCEEDED":56},"jobs":[{"dataWritten":0,"dataRead":4430,"rowCount":50,"jobId":62,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:24.338GMT","completionTime":"2023-06-26T05:10:24.368GMT","stageIds":[88,89,87],"jobGroup":"7","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4430,"dataRead":1903,"rowCount":54,"jobId":61,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:24.098GMT","completionTime":"2023-06-26T05:10:24.321GMT","stageIds":[85,86],"jobGroup":"7","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1903,"dataRead":2331,"rowCount":8,"jobId":60,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:23.960GMT","completionTime":"2023-06-26T05:10:24.000GMT","stageIds":[84],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":59,"name":"","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:23.526GMT","completionTime":"2023-06-26T05:10:23.526GMT","stageIds":[],"jobGroup":"7","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":56371746,"dataRead":148328958,"rowCount":4000000,"jobId":58,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:18.589GMT","completionTime":"2023-06-26T05:10:23.451GMT","stageIds":[82,83],"jobGroup":"7","status":"SUCCEEDED","numTasks":11,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":10,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":148328958,"dataRead":69530166,"rowCount":4000000,"jobId":57,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:17.271GMT","completionTime":"2023-06-26T05:10:18.554GMT","stageIds":[81],"jobGroup":"7","status":"SUCCEEDED","numTasks":10,"numActiveTasks":0,"numCompletedTasks":10,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":10,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":56,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:17.033GMT","completionTime":"2023-06-26T05:10:17.108GMT","stageIds":[80],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4400,"rowCount":50,"jobId":55,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:16.691GMT","completionTime":"2023-06-26T05:10:16.715GMT","stageIds":[78,79,77],"jobGroup":"7","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4400,"dataRead":1833,"rowCount":54,"jobId":54,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:16.456GMT","completionTime":"2023-06-26T05:10:16.678GMT","stageIds":[75,76],"jobGroup":"7","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":1833,"dataRead":2078,"rowCount":8,"jobId":53,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:16.262GMT","completionTime":"2023-06-26T05:10:16.304GMT","stageIds":[74],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":52,"name":"","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:15.849GMT","completionTime":"2023-06-26T05:10:15.849GMT","stageIds":[],"jobGroup":"7","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":9088547,"dataRead":10386298,"rowCount":200000,"jobId":51,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:15.184GMT","completionTime":"2023-06-26T05:10:15.778GMT","stageIds":[72,73],"jobGroup":"7","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":10386298,"dataRead":8063459,"rowCount":200000,"jobId":50,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:14.802GMT","completionTime":"2023-06-26T05:10:15.151GMT","stageIds":[71],"jobGroup":"7","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":49,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:14.580GMT","completionTime":"2023-06-26T05:10:14.634GMT","stageIds":[70],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4410,"rowCount":50,"jobId":48,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:14.185GMT","completionTime":"2023-06-26T05:10:14.210GMT","stageIds":[67,68,69],"jobGroup":"7","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4410,"dataRead":3002,"rowCount":56,"jobId":47,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:13.937GMT","completionTime":"2023-06-26T05:10:14.172GMT","stageIds":[66,65],"jobGroup":"7","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3002,"dataRead":3055,"rowCount":12,"jobId":46,"name":"toString at String.java:2994","description":"Delta: Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-06-26T05:10:13.745GMT","completionTime":"2023-06-26T05:10:13.788GMT","stageIds":[64],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":45,"name":"","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:13.288GMT","completionTime":"2023-06-26T05:10:13.288GMT","stageIds":[],"jobGroup":"7","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":428824814,"dataRead":566282367,"rowCount":16000000,"jobId":44,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:07.470GMT","completionTime":"2023-06-26T05:10:13.190GMT","stageIds":[63,62],"jobGroup":"7","status":"SUCCEEDED","numTasks":13,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":10,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":566282367,"dataRead":453483467,"rowCount":16000000,"jobId":43,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-26T05:10:04.544GMT","completionTime":"2023-06-26T05:10:07.430GMT","stageIds":[61],"jobGroup":"7","status":"SUCCEEDED","numTasks":10,"numActiveTasks":0,"numCompletedTasks":10,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":10,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ba7ee53e-89f3-4cc9-8eba-af60250a14ed"},"text/plain":"StatementMeta(, 0b346e6f-8846-4439-90cb-bdbb2139ac01, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"5b0917e6-0e49-47e9-beec-9dca4dd5ae58"},{"id":"82c15e68-5db3-4f73-98bf-1cee387141e5"}],"default_lakehouse_workspace_id":"e7bad9db-108e-49db-a0f0-f895ce565556","default_lakehouse":"82c15e68-5db3-4f73-98bf-1cee387141e5","default_lakehouse_name":"data"}}},"nbformat":4,"nbformat_minor":0}