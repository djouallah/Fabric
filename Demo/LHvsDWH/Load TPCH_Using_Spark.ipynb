{"cells":[{"cell_type":"markdown","source":["Create a new lakehouse called it TPCH100 or something like that, it is a one off manual operation, then add it to the current notebook"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5750846c-bc3a-42f4-b032-247147735573"},{"cell_type":"code","source":["!pip install -q duckdb --pre --upgrade"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ba58d74d-628c-42a8-a139-3b4c514a9b3f","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-29T05:27:22.8349792Z","session_start_time":"2023-10-29T05:27:23.9031389Z","execution_start_time":"2023-10-29T05:27:32.2942798Z","execution_finish_time":"2023-10-29T05:27:40.6888372Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"d31bb709-c92b-45d1-a114-95598e6ce765"},"text/plain":"StatementMeta(, ba58d74d-628c-42a8-a139-3b4c514a9b3f, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8271cfb7-d625-421d-83db-16735a5439b6"},{"cell_type":"code","source":["sf =100"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ba58d74d-628c-42a8-a139-3b4c514a9b3f","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-29T05:27:22.8355224Z","session_start_time":null,"execution_start_time":"2023-10-29T05:27:41.1237948Z","execution_finish_time":"2023-10-29T05:27:41.455977Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"58f099d3-df94-4bab-a3b6-3235cb9b9660"},"text/plain":"StatementMeta(, ba58d74d-628c-42a8-a139-3b4c514a9b3f, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"406e0f03-f43f-4db8-919e-5c68cc9c0856"},{"cell_type":"code","source":["%%time\n","import duckdb\n","import pathlib\n","for x in range(0, sf) :\n","  con=duckdb.connect()\n","  con.sql('PRAGMA disable_progress_bar;SET preserve_insertion_order=false')\n","  con.sql(f\"CALL dbgen(sf={sf} , children ={sf}, step = {x})\") \n","  for tbl in ['nation','region','customer','supplier','lineitem','orders','partsupp','part'] :\n","     pathlib.Path(f'/lakehouse/default/Files/{sf}/{tbl}').mkdir(parents=True, exist_ok=True) \n","     con.sql(f\"COPY (SELECT * FROM {tbl}) TO '/lakehouse/default/Files/{sf}/{tbl}/{x:03d}.parquet' \")\n","  con.close()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ba58d74d-628c-42a8-a139-3b4c514a9b3f","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-29T05:27:22.8359741Z","session_start_time":null,"execution_start_time":"2023-10-29T05:27:41.877818Z","execution_finish_time":"2023-10-29T05:27:42.1976029Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"51678166-707a-4940-8305-cb3e66568333"},"text/plain":"StatementMeta(, ba58d74d-628c-42a8-a139-3b4c514a9b3f, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"34dfdf71-8b2a-4def-9646-0ac255240c84"},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","from pyspark.sql.types import *\n","def loadFullDataFromSource(table_name):\n","    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n","    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","full_tables = [\n","    'customer',\n","    'lineitem',\n","    'nation',\n","    'orders' ,\n","    'region',\n","    'partsupp',\n","    'supplier' ,\n","    'part'\n","    ]\n","\n","for table in full_tables:\n","    loadFullDataFromSource(table)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"ba58d74d-628c-42a8-a139-3b4c514a9b3f","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-29T05:27:22.8364098Z","session_start_time":null,"execution_start_time":"2023-10-29T05:27:43.5915388Z","execution_finish_time":"2023-10-29T05:35:36.2498583Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":64,"FAILED":0},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4430,"rowCount":50,"usageDescription":"","jobId":71,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:35:34.916GMT","completionTime":"2023-10-29T05:35:34.949GMT","stageIds":[99,97,98],"jobGroup":"6","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4430,"dataRead":1908,"rowCount":54,"usageDescription":"","jobId":70,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:35:34.755GMT","completionTime":"2023-10-29T05:35:34.901GMT","stageIds":[96,95],"jobGroup":"6","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1908,"dataRead":2334,"rowCount":8,"usageDescription":"","jobId":69,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:35:34.462GMT","completionTime":"2023-10-29T05:35:34.650GMT","stageIds":[94],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":68,"name":"","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:35:34.061GMT","completionTime":"2023-10-29T05:35:34.061GMT","stageIds":[],"jobGroup":"6","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":523054282,"dataRead":1483296978,"rowCount":40000000,"usageDescription":"","jobId":67,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:34:46.845GMT","completionTime":"2023-10-29T05:35:33.935GMT","stageIds":[93,92],"jobGroup":"6","status":"SUCCEEDED","numTasks":101,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":100,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":1483296978,"dataRead":695255292,"rowCount":40000000,"usageDescription":"","jobId":66,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:34:45.503GMT","completionTime":"2023-10-29T05:34:46.796GMT","stageIds":[91],"jobGroup":"6","status":"SUCCEEDED","numTasks":100,"numActiveTasks":0,"numCompletedTasks":100,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":100,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"parquet at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":65,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:34:45.038GMT","completionTime":"2023-10-29T05:34:45.325GMT","stageIds":[90],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"parquet at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":64,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 100 paths:<br/>abfss://d8159ef4-5a62-4166-b350-be4db265b970@onelake.dfs.fabric.microsoft.com/7e0ee3e3-d578-4384-ba57-0efc95a41daa/Files/100/part/000.parquet, ...","submissionTime":"2023-10-29T05:34:44.864GMT","completionTime":"2023-10-29T05:34:44.993GMT","stageIds":[89],"jobGroup":"6","status":"SUCCEEDED","numTasks":100,"numActiveTasks":0,"numCompletedTasks":100,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":100,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4401,"rowCount":50,"usageDescription":"","jobId":63,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:34:44.655GMT","completionTime":"2023-10-29T05:34:44.716GMT","stageIds":[88,86,87],"jobGroup":"6","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4401,"dataRead":1833,"rowCount":54,"usageDescription":"","jobId":62,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:34:44.522GMT","completionTime":"2023-10-29T05:34:44.637GMT","stageIds":[84,85],"jobGroup":"6","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":1833,"dataRead":2091,"rowCount":8,"usageDescription":"","jobId":61,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:34:44.043GMT","completionTime":"2023-10-29T05:34:44.404GMT","stageIds":[83],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":60,"name":"","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:34:43.628GMT","completionTime":"2023-10-29T05:34:43.628GMT","stageIds":[],"jobGroup":"6","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":80272772,"dataRead":103878330,"rowCount":2000000,"usageDescription":"","jobId":59,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:34:40.903GMT","completionTime":"2023-10-29T05:34:43.507GMT","stageIds":[81,82],"jobGroup":"6","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":50,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":103878330,"dataRead":80646993,"rowCount":2000000,"usageDescription":"","jobId":58,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:34:40.464GMT","completionTime":"2023-10-29T05:34:40.870GMT","stageIds":[80],"jobGroup":"6","status":"SUCCEEDED","numTasks":50,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"parquet at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":57,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:34:40.221GMT","completionTime":"2023-10-29T05:34:40.284GMT","stageIds":[79],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"parquet at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":56,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 100 paths:<br/>abfss://d8159ef4-5a62-4166-b350-be4db265b970@onelake.dfs.fabric.microsoft.com/7e0ee3e3-d578-4384-ba57-0efc95a41daa/Files/100/supplier/000.parquet, ...","submissionTime":"2023-10-29T05:34:40.049GMT","completionTime":"2023-10-29T05:34:40.174GMT","stageIds":[78],"jobGroup":"6","status":"SUCCEEDED","numTasks":100,"numActiveTasks":0,"numCompletedTasks":100,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":100,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4424,"rowCount":50,"usageDescription":"","jobId":55,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:34:39.897GMT","completionTime":"2023-10-29T05:34:39.936GMT","stageIds":[75,76,77],"jobGroup":"6","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4424,"dataRead":3700,"rowCount":57,"usageDescription":"","jobId":54,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:34:39.754GMT","completionTime":"2023-10-29T05:34:39.879GMT","stageIds":[74,73],"jobGroup":"6","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":3700,"dataRead":3723,"rowCount":14,"usageDescription":"","jobId":53,"name":"toString at String.java:2994","description":"Delta: Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table): Compute snapshot for version: 0","submissionTime":"2023-10-29T05:34:39.580GMT","completionTime":"2023-10-29T05:34:39.638GMT","stageIds":[72],"jobGroup":"6","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":52,"name":"","description":"Job group for statement 6:\nspark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\nspark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-10-29T05:34:39.045GMT","completionTime":"2023-10-29T05:34:39.045GMT","stageIds":[],"jobGroup":"6","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"1f091149-d459-400c-8ca6-487d2b380c9b"},"text/plain":"StatementMeta(, ba58d74d-628c-42a8-a139-3b4c514a9b3f, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{},"id":"dac288b7-8693-42e2-a905-b868b25d372b"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"host":{}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"7e0ee3e3-d578-4384-ba57-0efc95a41daa"}],"default_lakehouse":"7e0ee3e3-d578-4384-ba57-0efc95a41daa","default_lakehouse_name":"LH100","default_lakehouse_workspace_id":"d8159ef4-5a62-4166-b350-be4db265b970"}}},"nbformat":4,"nbformat_minor":5}