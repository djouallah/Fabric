{"cells":[{"cell_type":"code","source":["!pip install duckdb --pre --upgrade"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["sf =100"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"8a782575-b5fc-487e-bec1-f4f4c2587530","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-15T00:42:58.1399176Z","session_start_time":null,"execution_start_time":"2023-06-15T00:42:58.5496981Z","execution_finish_time":"2023-06-15T00:42:58.8975904Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"8cb399b7-441e-44db-8872-28019198d845"},"text/plain":"StatementMeta(, 8a782575-b5fc-487e-bec1-f4f4c2587530, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["%%time\r\n","\r\n","import duckdb\r\n","import pathlib\r\n","for x in range(0, sf) :\r\n","  con=duckdb.connect()\r\n","  con.sql('PRAGMA disable_progress_bar;SET preserve_insertion_order=false')\r\n","  con.sql(f\"CALL dbgen(sf={sf} , children ={sf}, step = {x})\") \r\n","  for tbl in ['nation','region','customer','supplier','lineitem','orders','partsupp','part'] :\r\n","     pathlib.Path(f'/lakehouse/default/Files/{sf}/{tbl}').mkdir(parents=True, exist_ok=True) \r\n","     con.sql(f\"COPY (SELECT * FROM {tbl}) TO '/lakehouse/default/Files/{sf}/{tbl}/{x:03d}.parquet' \")\r\n","  con.close()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"2cdd069f-eb00-444e-bf73-d790ab9a82bc","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-11T04:44:44.62913Z","session_start_time":null,"execution_start_time":"2023-06-11T04:45:07.1199093Z","execution_finish_time":"2023-06-11T09:06:25.8548425Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"1c93f735-6397-415a-9a28-d7ed5f68ff8c"},"text/plain":"StatementMeta(, 2cdd069f-eb00-444e-bf73-d790ab9a82bc, 4, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 9h 22min 10s, sys: 28min 45s, total: 9h 50min 56s\nWall time: 4h 21min 16s\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"dff08c24-ef19-4512-a421-b2b82a922c91","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-15T12:44:36.7308292Z","session_start_time":"2023-06-15T12:44:36.9702617Z","execution_start_time":"2023-06-15T12:44:46.7761199Z","execution_finish_time":"2023-06-15T12:44:48.9201156Z","spark_jobs":{"numbers":{"UNKNOWN":0,"SUCCEEDED":0,"RUNNING":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"19758db6-8732-4263-8635-5f81fcce2e05"},"text/plain":"StatementMeta(, dff08c24-ef19-4512-a421-b2b82a922c91, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n","def loadFullDataFromSource(table_name):\n","    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n","    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","full_tables = [\n","    'customer',\n","    'lineitem',\n","    'nation',\n","    'orders' ,\n","    'region',\n","    'partsupp',\n","    'supplier' ,\n","    'part'\n","    ]\n","\n","for table in full_tables:\n","    loadFullDataFromSource(table)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"8a782575-b5fc-487e-bec1-f4f4c2587530","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-15T00:43:11.5986491Z","session_start_time":null,"execution_start_time":"2023-06-15T00:43:11.9458096Z","execution_finish_time":"2023-06-15T00:43:49.5158993Z","spark_jobs":{"numbers":{"RUNNING":0,"UNKNOWN":0,"FAILED":0,"SUCCEEDED":16},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":22,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    #df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-15T00:43:47.992GMT","completionTime":"2023-06-15T00:43:48.083GMT","stageIds":[25],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":21,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 1000 paths:<br/>abfss://bb82abdf-8e66-44a5-be95-0a0c20421794@onelake.dfs.fabric.microsoft.com/0e1e6461-b9f8-4966-a51c-9ea88ceeadca/Files/1000/part/000.parquet, ...","submissionTime":"2023-06-15T00:43:44.452GMT","completionTime":"2023-06-15T00:43:47.889GMT","stageIds":[24],"jobGroup":"5","status":"SUCCEEDED","numTasks":200,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":20,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    #df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-15T00:43:44.031GMT","completionTime":"2023-06-15T00:43:44.113GMT","stageIds":[23],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":19,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 1000 paths:<br/>abfss://bb82abdf-8e66-44a5-be95-0a0c20421794@onelake.dfs.fabric.microsoft.com/0e1e6461-b9f8-4966-a51c-9ea88ceeadca/Files/1000/supplier/000.parquet, ...","submissionTime":"2023-06-15T00:43:40.508GMT","completionTime":"2023-06-15T00:43:43.935GMT","stageIds":[22],"jobGroup":"5","status":"SUCCEEDED","numTasks":200,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":18,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    #df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-15T00:43:39.959GMT","completionTime":"2023-06-15T00:43:40.134GMT","stageIds":[21],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":17,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 1000 paths:<br/>abfss://bb82abdf-8e66-44a5-be95-0a0c20421794@onelake.dfs.fabric.microsoft.com/0e1e6461-b9f8-4966-a51c-9ea88ceeadca/Files/1000/partsupp/000.parquet, ...","submissionTime":"2023-06-15T00:43:36.189GMT","completionTime":"2023-06-15T00:43:39.867GMT","stageIds":[20],"jobGroup":"5","status":"SUCCEEDED","numTasks":200,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":16,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    #df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-15T00:43:35.819GMT","completionTime":"2023-06-15T00:43:35.879GMT","stageIds":[19],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":15,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 1000 paths:<br/>abfss://bb82abdf-8e66-44a5-be95-0a0c20421794@onelake.dfs.fabric.microsoft.com/0e1e6461-b9f8-4966-a51c-9ea88ceeadca/Files/1000/region/000.parquet, ...","submissionTime":"2023-06-15T00:43:31.828GMT","completionTime":"2023-06-15T00:43:35.732GMT","stageIds":[18],"jobGroup":"5","status":"SUCCEEDED","numTasks":200,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":14,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    #df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-15T00:43:31.329GMT","completionTime":"2023-06-15T00:43:31.404GMT","stageIds":[17],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":13,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 1000 paths:<br/>abfss://bb82abdf-8e66-44a5-be95-0a0c20421794@onelake.dfs.fabric.microsoft.com/0e1e6461-b9f8-4966-a51c-9ea88ceeadca/Files/1000/orders/000.parquet, ...","submissionTime":"2023-06-15T00:43:27.562GMT","completionTime":"2023-06-15T00:43:31.238GMT","stageIds":[16],"jobGroup":"5","status":"SUCCEEDED","numTasks":200,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":12,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    #df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-15T00:43:26.999GMT","completionTime":"2023-06-15T00:43:27.095GMT","stageIds":[15],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":11,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 1000 paths:<br/>abfss://bb82abdf-8e66-44a5-be95-0a0c20421794@onelake.dfs.fabric.microsoft.com/0e1e6461-b9f8-4966-a51c-9ea88ceeadca/Files/1000/nation/000.parquet, ...","submissionTime":"2023-06-15T00:43:23.265GMT","completionTime":"2023-06-15T00:43:26.887GMT","stageIds":[14],"jobGroup":"5","status":"SUCCEEDED","numTasks":200,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":10,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    #df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-15T00:43:22.714GMT","completionTime":"2023-06-15T00:43:22.888GMT","stageIds":[13],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":9,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 1000 paths:<br/>abfss://bb82abdf-8e66-44a5-be95-0a0c20421794@onelake.dfs.fabric.microsoft.com/0e1e6461-b9f8-4966-a51c-9ea88ceeadca/Files/1000/lineitem/000.parquet, ...","submissionTime":"2023-06-15T00:43:18.895GMT","completionTime":"2023-06-15T00:43:22.609GMT","stageIds":[12],"jobGroup":"5","status":"SUCCEEDED","numTasks":200,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":8,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 5:\nfrom pyspark.sql.types import *\ndef loadFullDataFromSource(table_name):\n    df = spark.read.parquet(f'Files/{sf}/' + table_name + '/*.parquet')\n    #df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\nfull_tables = [\n    'customer',\n    'lineitem',\n    'nation',\n    'orders' ,\n    'region',\n    'partsupp',\n    'supplier' ,\n    'part'\n    ]\n\nfor table in full_tables:\n    loadFullDataFromSource(table)","submissionTime":"2023-06-15T00:43:18.320GMT","completionTime":"2023-06-15T00:43:18.535GMT","stageIds":[11],"jobGroup":"5","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":7,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Listing leaf files and directories for 1000 paths:<br/>abfss://bb82abdf-8e66-44a5-be95-0a0c20421794@onelake.dfs.fabric.microsoft.com/0e1e6461-b9f8-4966-a51c-9ea88ceeadca/Files/1000/customer/000.parquet, ...","submissionTime":"2023-06-15T00:43:12.907GMT","completionTime":"2023-06-15T00:43:18.140GMT","stageIds":[10],"jobGroup":"5","status":"SUCCEEDED","numTasks":200,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"d5fe5000-a8eb-4e6b-86a0-34b2919c3206"},"text/plain":"StatementMeta(, 8a782575-b5fc-487e-bec1-f4f4c2587530, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"5b0917e6-0e49-47e9-beec-9dca4dd5ae58"}],"default_lakehouse_workspace_id":"bb82abdf-8e66-44a5-be95-0a0c20421794"}}},"nbformat":4,"nbformat_minor":0}