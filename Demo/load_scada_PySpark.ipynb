{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"b42bc406-fdf7-4289-bcc8-c5f6e794b2b0","known_lakehouses":[{"id":"b42bc406-fdf7-4289-bcc8-c5f6e794b2b0"}],"default_lakehouse_name":"AEMO","default_lakehouse_workspace_id":"cdf205b2-0d60-4b4a-9d6b-0b2abf3d1f5a"}}},"cells":[{"cell_type":"code","source":["import tempfile\n","import pandas as pd\n","import pyarrow as pa\n","import pyarrow.dataset as ds\n","import re ,shutil\n","from urllib.request import urlopen\n","import os\n","\n","def get_file_path(filename):\n","    return os.path.join(tempfile.gettempdir(), filename)\n","def load(Path):   \n","    \n","    appended_data = []\n","    url = \"http://nemweb.com.au/Reports/Current/Dispatch_SCADA/\"\n","    result = urlopen(url).read().decode('utf-8')\n","    pattern = re.compile(r'[\\w.]*.zip')\n","    filelist1 = pattern.findall(result)\n","    filelist_unique = dict.fromkeys(filelist1)\n","    filelist_sorted=sorted(filelist_unique, reverse=True)\n","    filelist = filelist_sorted[:600]\n","    try:\n","        df = ds.dataset(Path + \"/aemo/log/scada/part-0.parquet\").to_table().to_pandas()\n","    except:\n","        df=pd.DataFrame(columns=['file'])     \n","    file_loaded= df['file'].unique()\n","    #print (df)\n","\n","    current = file_loaded.tolist()\n","    #print(current)\n","\n","    files_to_upload = list(set(filelist) - set(current))\n","    files_to_upload = list(dict.fromkeys(files_to_upload)) \n","    print(str(len(files_to_upload)) + ' New File Loaded')\n","    if len(files_to_upload) != 0 :\n","      for x in files_to_upload:\n","            with urlopen(url+x) as source, open(get_file_path(x), 'w+b') as target:\n","                shutil.copyfileobj(source, target)\n","            df = pd.read_csv(get_file_path(x),skiprows=1,usecols=[\"SETTLEMENTDATE\", \"DUID\", \"SCADAVALUE\"],parse_dates=[\"SETTLEMENTDATE\"])\n","            df=df.dropna(how='all') #drop na\n","            df['SETTLEMENTDATE']= pd.to_datetime(df['SETTLEMENTDATE'])\n","            df['Date'] = df['SETTLEMENTDATE'].dt.date\n","            df['file'] = x\n","            appended_data.append(df)\n","            # see pd.concat documentation for more info\n","      appended_data = pd.concat(appended_data)\n","      existing_file = pd.DataFrame( file_loaded)\n","      new_file = pd.DataFrame(  appended_data['file'].unique())\n","      log = pd.concat ([new_file,existing_file], ignore_index=True)\n","      #print(log)\n","      log.rename(columns={0: 'file'}, inplace=True)\n","      \n","      log_tb=pa.Table.from_pandas(log,preserve_index=False)\n","      #print(log_tb)\n","      log_schema = pa.schema([pa.field('file', pa.string())])\n","      log_tb=log_tb.cast(target_schema=log_schema)\n","      \n","      tb=pa.Table.from_pandas(appended_data,preserve_index=False)\n","      my_schema = pa.schema([\n","                      pa.field('SETTLEMENTDATE', pa.timestamp('us')),\n","                      pa.field('DUID', pa.string()),\n","                      pa.field('SCADAVALUE', pa.float64()),\n","                      pa.field('Date', pa.date32()),\n","                      pa.field('file', pa.string())\n","                      ]\n","                                                       )\n","      xx=tb.cast(target_schema=my_schema)\n","      parquet_file_name=str((appended_data['SETTLEMENTDATE'].max()).strftime('%Y%m%d%X')).replace(\":\",\"\") + '{i}.parquet'\n","      ds.write_dataset(xx,Path + '/aemo/scada', format=\"parquet\" , partitioning=['Date'],\\\n","      partitioning_flavor=\"hive\",basename_template =parquet_file_name, max_rows_per_group=120000,existing_data_behavior=\"overwrite_or_ignore\")   \n","      ds.write_dataset(log_tb,Path +\"/aemo/log/scada\", format=\"parquet\" ,existing_data_behavior=\"overwrite_or_ignore\")\n","      return \"done\"\n","load(\"/lakehouse/default/Files\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"8996c09a-7859-41de-9038-a9bb61fa8433","statement_id":9,"state":"submitted","livy_statement_state":"running","queued_time":"2023-06-04T10:01:42.7643681Z","session_start_time":null,"execution_start_time":"2023-06-04T10:01:44.803563Z","execution_finish_time":null,"spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"e32e904f-69d0-4c51-b08b-9c053734e645"},"text/plain":"StatementMeta(, 8996c09a-7859-41de-9038-a9bb61fa8433, 9, Submitted, Running)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["77 New File Loaded\n"]}],"execution_count":7,"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n","from pyspark.sql.types import *\r\n","df = spark.read.parquet('Files/aemo/scada/*/*.parquet')\r\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/scada\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-06-04T10:01:42.8430014Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"2f4d1177-faa5-4cd2-b5cc-616ec81d8ca3"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}]}
