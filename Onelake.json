{"name":"Onelake","description":"","version":"1.0","culture":"en-AU","modifiedTime":"2023-06-06T03:16:56.5985751+00:00","ppdf:outputFileFormat":"Csv","pbi:mashup":{"fastCombine":false,"allowNativeQueries":false,"queriesMetadata":{"fn_ReadDeltaTable":{"queryId":"71af1d91-269e-4e90-938f-82c7ae5189c3","queryName":"fn_ReadDeltaTable"},"onelake":{"queryId":"ab2d3019-7865-4dac-b2f2-586937d3a75d","queryName":"onelake"},"nation":{"queryId":"a3b6de2c-ec1d-49b3-a358-359d88f288c6","queryName":"nation","loadEnabled":true}},"document":"section Section1;\r\nshared fn_ReadDeltaTable = let\n    fn_ReadDeltaTable = (\n        DeltaTableFolderContent as table,\n        optional DeltaTableOptions as record\n    ) as table =>\n\n    let\n\n        DeltaTableVersion = if DeltaTableOptions = null then null else Record.FieldOrDefault(DeltaTableOptions, \"Version\", null),\n        PartitionFilterFunction = if DeltaTableOptions = null then (x) => true else if Record.FieldOrDefault(DeltaTableOptions, \"PartitionFilterFunction\", null) = null then (x) => true else Record.Field(DeltaTableOptions, \"PartitionFilterFunction\"),\n        StatsFilterFunction = if DeltaTableOptions = null then (x, y) => true else if Record.FieldOrDefault(DeltaTableOptions, \"StatsFilterFunction\", null) = null then (x, y) => true else Record.Field(DeltaTableOptions, \"StatsFilterFunction\"),\n        UseFileBuffer = if DeltaTableOptions = null then false else if Record.FieldOrDefault(DeltaTableOptions, \"UseFileBuffer\", null) = null then false else Record.Field(DeltaTableOptions, \"UseFileBuffer\"),\n        IterateFolderContent = if DeltaTableOptions = null then false else if Record.FieldOrDefault(DeltaTableOptions, \"IterateFolderContent\", null) = null then false else Record.Field(DeltaTableOptions, \"IterateFolderContent\"),\n        TimeZoneOffset = if DeltaTableOptions = null then null else Record.FieldOrDefault(DeltaTableOptions, \"TimeZoneOffset\", null),\n        TimeZoneOffsetDuration = Duration.FromText(Text.TrimStart(TimeZoneOffset, \"+\")),\n\n        Delimiter = if Text.Contains(DeltaTableFolderContent{0}[Folder Path], \"//\") then \"/\" else \"\\\",\n\n        DeltaProtocol =\n        let\n            Source = #\"Logs ALL\",\n            #\"Filtered Rows1\" = Table.SelectRows(Source, each ([protocol] <> null)),\n            MaxVersion = Table.Group(#\"Filtered Rows1\", {}, {{\"MaxVersion\", each List.Max([Version]), type number}}){0}[MaxVersion],\n            #\"Filtered Rows2\" = Table.SelectRows(#\"Filtered Rows1\", each [Version] = MaxVersion),\n            #\"Removed Other Columns\" = Table.SelectColumns(#\"Filtered Rows2\",{\"protocol\"}),\n            #\"Expanded protocol\" = Table.ExpandRecordColumn(#\"Removed Other Columns\", \"protocol\", {\"minReaderVersion\", \"minWriterVersion\"}, {\"minReaderVersion\", \"minWriterVersion\"}),\n            #\"Changed Type\" = Table.TransformColumnTypes(#\"Expanded protocol\",{{\"minReaderVersion\", Int64.Type}, {\"minWriterVersion\", Int64.Type}}),\n            #\"Renamed Columns\" = Table.Buffer(#\"Changed Type\")\n        in\n            #\"Renamed Columns\",\n\n        DeltaTableFolderContent_wFullPath = \n        let\n            Source = DeltaTableFolderContent,\n\n            fn_ReadContentRecursive = (tbl as table) as table => \n                let\n                    subFolders = Table.SelectRows(tbl, each Value.Is(_[Content], type table)),\n                    binaries = Table.SelectRows(tbl, each Value.Is(_[Content], type binary)),\n                    combinedContent = if Table.RowCount(subFolders) > 0 then Table.Combine({binaries, @fn_ReadContentRecursive(Table.Combine(subFolders[Content]))}) else binaries\n                in\n                    combinedContent,\n\n            Content = if IterateFolderContent then fn_ReadContentRecursive(Source) else Source,\n\n            #\"Added Full_Path\" = Table.AddColumn(Content, \"Full_Path\", each Text.Replace([Folder Path] & [Name], \"=\", \"%3D\"), Text.Type),\n            #\"Added File_Name\" = Table.AddColumn(#\"Added Full_Path\", \"File_Name\", each if Text.Length([Extension]) > 0 then List.Last(Text.Split([Full_Path], Delimiter)) else null, type text),\n            Buffered = Table.Buffer(#\"Added File_Name\")\n        in\n            Buffered,\n\n        PQ_DataTypes = \n        let\n            Source = [\n                Any.Type = Any.Type,\n                None.Type = None.Type,\n                Day.Type = Day.Type,\n                Duration.Type = Duration.Type,\n                Record.Type = Record.Type,\n                Precision.Type = Precision.Type,\n                Number.Type = Number.Type,\n                Binary.Type = Binary.Type,\n                Byte.Type = Byte.Type,\n                Character.Type = Character.Type,\n                Text.Type = Text.Type,\n                Function.Type = Function.Type,\n                Null.Type = Null.Type,\n                List.Type = List.Type,\n                Type.Type = Type.Type,\n                Logical.Type = Logical.Type,\n                Int8.Type = Int8.Type,\n                Int16.Type = Int16.Type,\n                Int32.Type = Int32.Type,\n                Int64.Type = Int64.Type,\n                Single.Type = Single.Type,\n                Double.Type = Double.Type,\n                Decimal.Type = Decimal.Type,\n                Currency.Type = Currency.Type,\n                Percentage.Type = Percentage.Type,\n                Guid.Type = Guid.Type,\n                Date.Type = Date.Type,\n                DateTime.Type = DateTime.Type,\n                DateTimeZone.Type = DateTimeZone.Type,\n                Time.Type = Time.Type,\n                Table.Type = Table.Type\n            ]\n        in\n        Source,\n\n        #\"TableSchema\" = \n        let\n            ExpressionText = \"type table [\" & Text.Combine(metadata_columns[TableDataType], \", \") & \"]\",\n            BufferedExpression = List.Buffer({ExpressionText}){0},\n            TableSchema = Expression.Evaluate(BufferedExpression, PQ_DataTypes)\n        in\n            TableSchema,\n\n        #\"PhysicalTableSchema\" =\n        let\n            ExpressionText = \"type table [\" & Text.Combine(metadata_columns[PhysicalTableDataType], \", \") & \"]\",\n            BufferedExpression = List.Buffer({ExpressionText}){0},\n            PhysicalTableSchema = Expression.Evaluate(BufferedExpression, PQ_DataTypes)\n        in\n            PhysicalTableSchema,\n\n        LogSchema = type [txn=record, add=record, remove=record, metaData=record, commitInfo=record, protocol=record],\n\n        #\"_delta_log Folder\" = \n        let\n            Source = DeltaTableFolderContent_wFullPath,\n            #\"Filtered Rows\" = Table.SelectRows(Source, each Text.Contains([Full_Path], Delimiter & \"_delta_log\" & Delimiter)),\n            DeltaLogValidated = if Table.RowCount(#\"Filtered Rows\") = 0 then error \"Mandatory folder \" & Delimiter & \"_delta_log\" & Delimiter & \" not found in the root of the file listing! Are you sure this is a Delta Lake table?\" else #\"Filtered Rows\",\n            #\"Added Version\" = Table.AddColumn(DeltaLogValidated, \"Version\", each try Int64.From(Text.BeforeDelimiter([File_Name], \".\")) otherwise -1, Int64.Type),\n            MaxVersion = Table.Group(#\"Added Version\", {}, {{\"MaxVersion\", each List.Max([Version]), type number}}){0}[MaxVersion],\n            #\"Filtered RequestedVersion\" = if DeltaTableVersion = null then #\"Added Version\" \n        else if DeltaTableVersion < 0 then Table.SelectRows(#\"Added Version\", each [Version] <= MaxVersion + DeltaTableVersion)\n        else Table.SelectRows(#\"Added Version\", each [Version] <= DeltaTableVersion),\n            BufferedTable = Table.Buffer(#\"Filtered RequestedVersion\"),\n            BufferedContent = Table.TransformColumns(BufferedTable,{{\"Content\", Binary.Buffer}})\n        in\n            BufferedContent,\n\n        #\"DeltaTablePath\" = \n        let\n            DeltaTablePath = Text.Combine(List.RemoveLastN(Text.Split(#\"_delta_log Folder\"{0}[Full_Path], Delimiter), 2), Delimiter) & Delimiter\n        in\n            DeltaTablePath,\n\n        #\"_last_checkpoint\" = \n        let\n            #\"_delta_log\" = #\"_delta_log Folder\",\n            #\"Filtered Rows\" = Table.SelectRows(_delta_log, each Text.EndsWith([Name], \"_last_checkpoint\")),\n            #\"Added Custom\" = Table.AddColumn(#\"Filtered Rows\", \"JsonContent\", each Json.Document([Content])),\n            JsonContent = #\"Added Custom\"{0}[JsonContent],\n            CheckEmpty = if Table.RowCount(#\"Filtered Rows\") = 0 then [Size=-1, version=-1] else JsonContent,\n            LatestCheckPointWithParts = if Record.HasFields(CheckEmpty, \"parts\") then CheckEmpty else Record.AddField(CheckEmpty, \"parts\", 1),\n\n            #\"Filtered Rows Version\" = Table.SelectRows(#\"_delta_log\", each Text.EndsWith([Name], \".checkpoint.parquet\")),\n            MaxVersion = try Table.Group(#\"Filtered Rows Version\", {}, {{\"MaxVersion\", each List.Max([Version]), type number}}){0}[MaxVersion] otherwise -1,\n            #\"Filtered Rows MaxVersion\" = Table.SelectRows(#\"Filtered Rows Version\", each [Version] = MaxVersion),\n            CheckpointFromVersion = [version=try MaxVersion otherwise -1, size=-1, parts = Table.RowCount(#\"Filtered Rows MaxVersion\")],\n\n            LastCheckpoint = Table.Buffer(Table.FromRecords({if DeltaTableVersion = null then LatestCheckPointWithParts else CheckpointFromVersion})){0}\n        in\n            LastCheckpoint,\n\n        #\"Checkpoint Files\" = \n        let\n            LastCheckpointFile = {1..Record.Field(_last_checkpoint, \"parts\")},\n            #\"Converted to Table\" = Table.FromList(LastCheckpointFile, Splitter.SplitByNothing(), {\"part\"}, null, ExtraValues.Error),\n            #\"Add Version\" = Table.AddColumn(#\"Converted to Table\", \"version\", each Record.Field(_last_checkpoint, \"version\")),\n            #\"Add SingleFile\" = Table.AddColumn(#\"Add Version\", \"file_name\", each Text.PadStart(Text.From([version]), 20, \"0\") & \".checkpoint.parquet\", Text.Type),\n            #\"Add MultipleFiles\" = Table.AddColumn(#\"Add Version\", \"file_name\", each Text.PadStart(Text.From([version]), 20, \"0\") & \".checkpoint.\" & Text.PadStart(Text.From([part]), 10, \"0\") & \".\" & Text.PadStart(Text.From(Record.Field(_last_checkpoint, \"parts\")), 10, \"0\") & \".parquet\", Text.Type),\n            AllFiles = Table.SelectColumns(if Record.Field(_last_checkpoint, \"parts\") = 1 then #\"Add SingleFile\" else #\"Add MultipleFiles\", \"file_name\"),\n            AllFiles_BufferedList = List.Buffer(Table.ToList(AllFiles)),\n            Content = Table.SelectRows(#\"_delta_log Folder\", each List.Count(List.Select(AllFiles_BufferedList, (inner) => Text.EndsWith([Name], inner))) > 0)\n        in\n            Content,\n\n        #\"Logs Checkpoint\" = \n        let\n            Source = #\"Checkpoint Files\",\n            #\"Parsed Logs\" = Table.AddColumn(Source, \"LogInfo\", each Parquet.Document([Content])),\n            #\"Combine LogInfo and Version\" = Table.Combine(Table.TransformRows(#\"Parsed Logs\", each fn_AddColumnsToTable([Version=_[Version]], _[LogInfo])))\n        in\n            #\"Combine LogInfo and Version\",\n\n        #\"Latest Log Files\" = \n        let\n            Source = #\"_delta_log Folder\",\n            #\"Filtered Rows\" = Table.SelectRows(Source, each ([Extension] = \".json\")),\n            #\"Filtered Rows1\" = Table.SelectRows(#\"Filtered Rows\", each [Version] > Record.Field(_last_checkpoint, \"version\"))\n        in\n            #\"Filtered Rows1\",\n\n        #\"Logs JSON\" = \n        let\n            Source = #\"Latest Log Files\",\n            #\"Added Custom\" = Table.AddColumn(Source, \"JsonContent\", each Lines.FromBinary([Content])),\n            #\"Expanded JsonContent\" = Table.ExpandListColumn(#\"Added Custom\", \"JsonContent\"),\n            #\"Parsed Logs\" = Table.TransformColumns(#\"Expanded JsonContent\",{{\"JsonContent\", Json.Document}}),\n            #\"Expanded Logs\" = Table.ExpandRecordColumn(#\"Parsed Logs\", \"JsonContent\", {\"add\", \"remove\", \"metaData\", \"commitInfo\", \"protocol\"}),\n            #\"Removed Other Columns\" = Table.SelectColumns(#\"Expanded Logs\",{\"Version\", \"add\", \"remove\", \"metaData\", \"commitInfo\", \"protocol\"})\n        in\n            #\"Removed Other Columns\",\n\n        #\"Logs ALL\" = \n        let\n            Source = Table.Combine({#\"Logs Checkpoint\", #\"Logs JSON\"}),\n            #\"Added timestamp\" = Table.AddColumn(Source, \"log_timestamp\", each if [add] <> null then Record.Field([add], \"modificationTime\") else \n        if [remove] <> null then Record.Field([remove], \"deletionTimestamp\") else \n        if [commitInfo] <> null then Record.Field([commitInfo], \"timestamp\") else \n        if [metaData] <> null then Record.Field([metaData], \"createdTime\") else null, Int64.Type),\n            #\"Added datetime\" = Table.AddColumn(#\"Added timestamp\", \"log_datetime\", each try #datetime(1970,1,1,0,0,0)+#duration(0,0,0,[log_timestamp]/1000) otherwise null, DateTime.Type)\n        in\n            #\"Added datetime\",\n\n        fn_GetPowerBIDataTypeInformation = \n        (type_value as any, physical_name as logical, optional is_nullable as nullable logical) as text =>\n        let \n            par_is_nullable = if is_nullable = null then true else is_nullable,\n\n            ret = if Value.Is(type_value, Record.Type) then \n                    if type_value[type] = \"struct\" then \"[\" & Text.Combine(List.Transform(type_value[fields], each \"#\"\"\" & (if(physical_name) then _[metadata][delta.columnMapping.physicalName] else _[name]) & \"\"\" = \" & @fn_GetPowerBIDataTypeInformation(_[type], physical_name, _[nullable])), \", \") & \"]\"\n                    else if type_value[type] = \"array\" then \"{\" & @fn_GetPowerBIDataTypeInformation(type_value[elementType], physical_name, type_value[containsNull]) & \"}\"\n                    else if type_value[type] = \"map\" then \"table [Key=\" & @fn_GetPowerBIDataTypeInformation(type_value[keyType], false) & \", Value=\" & @fn_GetPowerBIDataTypeInformation(type_value[valueType], physical_name, type_value[valueContainsNull]) & \"]\"\n                    else \"Any.Type\"\n                else if type_value = \"string\" then \"Text.Type\"\n                else if type_value = \"long\" then \"Int64.Type\"\n                else if type_value = \"integer\" then \"Int32.Type\"\n                else if type_value = \"short\" then \"Int16.Type\"\n                else if type_value = \"byte\" then \"Int8.Type\"\n                else if type_value = \"float\" then \"Single.Type\"\n                else if type_value = \"double\" then \"Double.Type\"\n                else if type_value = \"date\" then \"Date.Type\"\n                else if type_value = \"timestamp\" and TimeZoneOffset = null then \"DateTime.Type\"\n                else if type_value = \"timestamp\" and TimeZoneOffset <> null then \"DateTimeZone.Type\"\n                else if type_value = \"boolean\" then \"Logical.Type\"\n                else if type_value = \"binary\" then \"Binary.Type\"\n                else if type_value = \"null\" then \"Any.Type\"\n                else if Text.StartsWith(type_value, \"decimal\") then \"Number.Type\"                \n                else \"Any.Type\",\n\n            ret_nullable = (if par_is_nullable then \"nullable \" else \"\") & ret\n        in\n            ret_nullable,\n\n        #\"metadata_columns\" = \n        let\n            Source = #\"Logs ALL\",\n            #\"Filtered Rows1\" = Table.SelectRows(Source, each ([metaData] <> null)),\n            MaxVersion = Table.Group(#\"Filtered Rows1\", {}, {{\"MaxVersion\", each List.Max([Version]), type number}}){0}[MaxVersion],\n            #\"Filtered Rows2\" = Table.SelectRows(#\"Filtered Rows1\", each [Version] = MaxVersion),\n            #\"Kept First Rows\" = Table.FirstN(#\"Filtered Rows2\",1),\n            #\"Removed Other Columns\" = Table.SelectColumns(#\"Kept First Rows\",{\"metaData\"}),\n            #\"Expanded metaData\" = Table.ExpandRecordColumn(#\"Removed Other Columns\", \"metaData\", {\"schemaString\", \"partitionColumns\"}, {\"schemaString\", \"partitionColumns\"}),\n            #\"Filtered Rows\" = Table.SelectRows(#\"Expanded metaData\", each ([schemaString] <> null)),\n            JSON = Table.TransformColumns(#\"Filtered Rows\",{{\"schemaString\", Json.Document}}),\n            #\"Expanded schemaString\" = Table.ExpandRecordColumn(JSON, \"schemaString\", {\"fields\"}, {\"fields\"}),\n            #\"Expanded fieldList\" = Table.ExpandListColumn(#\"Expanded schemaString\", \"fields\"),\n            #\"Expanded fields\" = Table.ExpandRecordColumn(#\"Expanded fieldList\", \"fields\", {\"name\", \"type\", \"nullable\", \"metadata\"}, {\"name\", \"type\", \"nullable\", \"metadata\"}),\n            #\"Added physicalName\" = Table.AddColumn(#\"Expanded fields\", \"physicalName\", each try Record.Field([metadata], \"delta.columnMapping.physicalName\") otherwise [name], type text),\n            #\"Changed Type\" = Table.TransformColumnTypes(#\"Added physicalName\",{{\"name\", type text}, {\"nullable\", type logical}}),\n            #\"Added isPartitionedBy\" = Table.Buffer(Table.AddColumn(#\"Changed Type\", \"isPartitionedBy\", each List.Contains([partitionColumns], [name]), Logical.Type)),\n            #\"Added PBI_Text\" = Table.AddColumn(#\"Added isPartitionedBy\", \"PBI_Text\", each fn_GetPowerBIDataTypeInformation([type], false, [nullable]), type text),\n            #\"Added PBI_DataType\" = Table.AddColumn(#\"Added PBI_Text\", \"PBI_DataType\", each Expression.Evaluate(\"type \" & [PBI_Text], PQ_DataTypes), type type),\n            #\"Added PBI_Transformation\" = Table.AddColumn(#\"Added PBI_DataType\", \"PBI_Transformation\", each \n                        if [type] = \"string\" then Text.From\n                        else if [type] = \"long\" then Int64.From\n                        else if [type] = \"integer\" then Int32.From\n                        else if [type] = \"short\" then Int16.From\n                        else if [type] = \"byte\" then Int8.From\n                        else if [type] = \"float\" then Single.From\n                        else if [type] = \"double\" then Double.From\n                        else if [type] = \"date\" then Date.From\n                        else if [type] = \"timestamp\" and TimeZoneOffset = null then DateTime.From\n                        else if [type] = \"timestamp\" and TimeZoneOffset <> null then (x) as nullable datetimezone => DateTime.AddZone(x + TimeZoneOffsetDuration, Duration.Hours(TimeZoneOffsetDuration), Duration.Minutes(TimeZoneOffsetDuration))\n                        else if [type] = \"boolean\" then Logical.From\n                        else if [type] = \"binary\" then Binary.From\n                        else if (Value.Is([type], type text) and Text.StartsWith([type], \"decimal\")) then Number.From\n                        else (x) as nullable any => x, type function),\n            #\"Added physicalPBI_Text\" = Table.AddColumn(#\"Added PBI_Transformation\", \"physicalPBI_Text\", each fn_GetPowerBIDataTypeInformation([type], true, [nullable]), type text),\n            #\"Added physicalPBI_DataType\" = Table.AddColumn(#\"Added physicalPBI_Text\", \"physicalPBI_DataType\", each Expression.Evaluate(\"type \" & [physicalPBI_Text], PQ_DataTypes), type type),\n            #\"Added ChangeDataType\" = Table.AddColumn(#\"Added physicalPBI_DataType\", \"ChangeDataType\", each {[name], [PBI_DataType]}, type list),\n            #\"Added TableDataType\" = Table.AddColumn(#\"Added ChangeDataType\", \"TableDataType\", each \"#\"\"\" & [name] & \"\"\"=\" & Text.From([PBI_Text]), type text),\n            #\"Added PhysicalTableDataType\" = Table.AddColumn(#\"Added TableDataType\", \"PhysicalTableDataType\", each \"#\"\"\" & [physicalName] & \"\"\"=\" & Text.From([PBI_Text]), type text),\n            #\"Added ColumnTransformation\" = Table.AddColumn(#\"Added PhysicalTableDataType\", \"ColumnTransformation\", each {[physicalName], [PBI_Transformation]}, type list),\n            #\"Buffered Fields\" = Table.Buffer(#\"Added ColumnTransformation\")\n        in\n            #\"Buffered Fields\",\n\n        fn_AddColumnsToTable = \n        (cols as record, tbl as table) as table =>\n        let \n            colName = List.First(Record.FieldNames(cols)),\n            cols_new = Record.RemoveFields(cols, colName),\n            tbl_new = Table.AddColumn(tbl, colName, (x) => Record.Field(cols, colName), Value.Type(Record.Field(cols, colName))),\n\n            ret = if Record.FieldCount(cols) = 0 then tbl else if Record.FieldCount(cols_new) = 0 then tbl_new else @fn_AddColumnsToTable(cols_new, tbl_new)\n        in\n            ret,\n\n        #\"Files with Stats\" = \n        let\n            Source = #\"Logs ALL\",\n            #\"Added Counter\" = Table.AddColumn(Source, \"Counter\", each if [remove] <> null then -1 else if [add] <> null then 1 else null, Int8.Type),\n            #\"Added file_name\" = Table.AddColumn(#\"Added Counter\", \"file_name\", each if [add] <> null then Record.Field([add], \"path\") else if [remove] <> null then Record.Field([remove], \"path\") else null, Text.Type),\n            #\"Filtered Rows\" = Table.SelectRows(#\"Added file_name\", each ([file_name] <> null)),\n            #\"Added partitionValuesTable\" = Table.AddColumn(#\"Filtered Rows\", \"partitionValuesTable\", each if [add] <> null then if Value.Is(Record.Field([add], \"partitionValues\"), Record.Type) then Record.ToTable(Record.Field([add], \"partitionValues\")) else Table.RenameColumns(Record.Field([add], \"partitionValues\"), {\"Key\", \"Name\"}) else null, type nullable table),\n            #\"Added partitionValuesJSON\" = Table.AddColumn(#\"Added partitionValuesTable\", \"partitionValuesJSON\", each Text.FromBinary(Json.FromValue([partitionValuesTable]))),\n            #\"Added stats\" = Table.AddColumn(#\"Added partitionValuesJSON\", \"stats\", each if [add] <> null then \n                if Value.Is(Record.Field([add], \"stats\"), type text) \n                then Record.Field([add], \"stats\") \n                else \"{}\"\n                else null, type text),\n            #\"Grouped Rows1\" = Table.Group(#\"Added stats\", {\"file_name\"}, {{\"partitionValuesJSON\", each List.Max([partitionValuesJSON]), type nullable text}, {\"stats\", each List.Max([stats]), type nullable text}, {\"isRelevant\", each List.Sum([Counter]), type nullable text}}),\n            #\"Relevant Files\" = Table.SelectRows(#\"Grouped Rows1\", each ([isRelevant] > 0)),\n            #\"Added partitionValuesTable2\" = Table.AddColumn(#\"Relevant Files\", \"partitionValuesTable\", each try Table.FromRecords(Json.Document([partitionValuesJSON])) otherwise null),\n            #\"Added partitionValuesRecord\" = Table.AddColumn(#\"Added partitionValuesTable2\", \"partitionValuesRecord\", each Record.TransformFields(Record.FromTable([partitionValuesTable]), Table.SelectRows(#\"metadata_columns\", each [isPartitionedBy] = true)[ColumnTransformation]), Expression.Evaluate(\"type [\" & Text.Combine(Table.SelectRows(#\"metadata_columns\", each [isPartitionedBy] = true)[TableDataType], \", \") & \"]\", PQ_DataTypes)),\n            #\"Expanded partitionValuesRecord\" = Table.ExpandRecordColumn(#\"Added partitionValuesRecord\", \"partitionValuesRecord\", Table.SelectRows(#\"metadata_columns\", each [isPartitionedBy] = true)[physicalName]),\n            #\"Parse stats to JSON\" = Table.AddColumn(#\"Expanded partitionValuesRecord\", \"JSON\", each Json.Document([stats]), type [minValues=list, maxValues=list, numRecords=Int64.Type, nullCount=Int64.Type]),\n            #\"Expanded Stats\" = Table.ExpandRecordColumn(#\"Parse stats to JSON\", \"JSON\", {\"minValues\", \"maxValues\", \"numRecords\", \"nullCount\"}, {\"minValues\", \"maxValues\", \"numRecords\", \"nullCount\"}),\n            #\"Removed Columns\" = Table.RemoveColumns(#\"Expanded Stats\",{\"partitionValuesJSON\", \"stats\", \"isRelevant\", \"partitionValuesTable\"}),\n            #\"Renamed for Filters\" = Table.RenameColumns(#\"Removed Columns\",Table.ToRows(Table.SelectColumns(Table.SelectRows(metadata_columns, each [isPartitionedBy]),{\"physicalName\", \"name\"}))),\n            #\"Apply PartitionFilterFunction\" = Table.SelectRows(#\"Renamed for Filters\", each PartitionFilterFunction(_)),\n            #\"Apply StatsFilterFunction\" = Table.SelectRows(#\"Apply PartitionFilterFunction\", each StatsFilterFunction([minValues], [maxValues])),\n            #\"Renamed to physicalNames\" = Table.RenameColumns(#\"Apply StatsFilterFunction\",Table.ToRows(Table.SelectColumns(Table.SelectRows(metadata_columns, each [isPartitionedBy]),{\"name\", \"physicalName\"})))\n        in\n            #\"Renamed to physicalNames\",\n\n    #\"Data\" = \n        let\n            #\"Added Full_Path\" = Table.AddColumn(#\"Files with Stats\", \"Full_Path\", each Text.Replace(DeltaTablePath & Text.Replace([file_name], \"=\", \"%3D\"), \"/\", Delimiter), Text.Type),\n            #\"Removed FilteringColumns\" = Table.RemoveColumns(#\"Added Full_Path\",{\"minValues\", \"maxValues\", \"numRecords\", \"nullCount\"}),\n            #\"Buffered RelevantFiles\" = Table.Buffer(#\"Removed FilteringColumns\"),\n            #\"Merged Queries\" = Table.NestedJoin(#\"Buffered RelevantFiles\", {\"Full_Path\"}, DeltaTableFolderContent_wFullPath, {\"Full_Path\"}, \"DeltaTable Folder\", JoinKind.Inner),\n            #\"Removed Full_Path\" = Table.RemoveColumns(#\"Merged Queries\",{\"Full_Path\"}),\n            #\"Expanded DeltaTable Folder\" = Table.ExpandTableColumn(#\"Removed Full_Path\", \"DeltaTable Folder\", {\"Content\"}, {\"Content\"}),\n            BufferFile = if UseFileBuffer then Table.TransformColumns(#\"Expanded DeltaTable Folder\",{{\"Content\", Binary.Buffer}}) else #\"Expanded DeltaTable Folder\",\n            #\"Read Parquet\" = Table.AddColumn(BufferFile, \"Data\", each Parquet.Document([Content]), type table),\n            #\"Removed Binary Column\" = Table.RemoveColumns(#\"Read Parquet\",{\"Content\"}),\n            #\"Combine Partition Values\" = Table.CombineColumnsToRecord(#\"Removed Binary Column\", \"cols\", List.RemoveItems(Table.ColumnNames(#\"Removed Binary Column\"), {\"Data\"})),\n            #\"Combine Files\" = Table.Combine(Table.TransformRows(#\"Combine Partition Values\", each fn_AddColumnsToTable(_[cols], _[Data])), PhysicalTableSchema),\n            #\"Changed Type\" = Table.TransformColumns(#\"Combine Files\",Table.SelectRows(metadata_columns, each [type] = \"timestamp\")[ColumnTransformation]),\n            #\"Table with TimeZoneOffset\" = if TimeZoneOffset = null then #\"Combine Files\" else #\"Changed Type\",\n            #\"Reordered Columns\" = Table.ReorderColumns(#\"Table with TimeZoneOffset\", metadata_columns[physicalName]),\n            #\"Renamed Columns\" = Table.RenameColumns(#\"Reordered Columns\",Table.ToRows(Table.SelectColumns(metadata_columns,{\"physicalName\", \"name\"}))),\n            #\"Renamed ComplexTypes\" = #\"Renamed Columns\" //Table.TransformColumnTypes(#\"Renamed Columns\",Table.ToRows(Table.SelectColumns(Table.SelectRows(metadata_columns, each [name] = \"structColumn\"),{\"name\", \"PBI_DataType\"})))\n        //Table.TransformColumnTypes(#\"Renamed Columns\",Table.ToRows(Table.SelectColumns(Table.SelectRows(metadata_columns, each [name] = \"structColumn\"),{\"name\", \"PBI_DataType\"})))\n        ,\n\n            #\"Validate ReaderVersion\" = if DeltaProtocol{0}[minReaderVersion] <= 2 then #\"Renamed ComplexTypes\" else error Error.Record(\"DeltaLakeVersionNotSupported\", \"This Connector currently only supports DeltaLake tables up to version 2.\", \"minReaderVersion\"),\n            FinalDeltaTable = Table.View(\n                #\"Validate ReaderVersion\", \n                [\n                    GetType = () => TableSchema,\n                    GetRowCount = () => List.Sum(#\"Files with Stats\"[numRecords])\n                    //,OnSelectRows = (condition) => Table.FirstN(#\"Reordered Columns\", 3)\n                ]\n            )\n        in\n            FinalDeltaTable\n            \n    in \n        #\"Data\",\n\n    documentation = [\n        Documentation.Name =  \"fn_ReadDeltaTable\",\n        Documentation.Description = \"Takes the file/folder list of a Delta Lake table and returns the content as a table object in Power Query.\",\n        Documentation.LongDescription = \"Takes the file/folder list of a Delta Lake table and returns the content as a table object in Power Query. An optional 2nd parameter can be used to for special features like Time Travel, Partition Elimination, etc.\",\n        Documentation.Category = \"Table\",\n        Documentation.Source = \"https://github.com/delta-io/connectors/blob/master/powerbi/fn_ReadDeltaTable.pq\",\n        Documentation.Version = \"1.0\",\n        Documentation.Author = \"Gerhard Brueckl, paiqo GmbH\",\n        Documentation.Examples = {[Description =  \"Reading Delta Lake table from Azure Blob Storage with Time-Travel\",\n            Code = \"let\n    Source = AzureStorage.Blobs(\"\"https://gbadls01.blob.core.windows.net/public\"\"),\n    #\"\"Filtered Rows\"\" = Table.SelectRows(Source, each Text.StartsWith([Name], \"\"powerbi_delta/FactInternetSales_part.delta/\"\")),\n    DeltaTable = fn_ReadDeltaTable(#\"\"Filtered Rows\"\", [Version=7])\nin\n    DeltaTable\",\n            Result = \"#table( {\"\"ProductKey\"\", \"\"OrderDateKey\"\", \"\"Value\"\"}, { {\"\"A\"\", \"\"2020-01-01\"\", 123} ,{\"\"B\"\", \"\"2020-04-02\"\", 45} } )\"],\n            \n            [Description =  \"Reading Delta Lake table from Azure Data Lake Storage Gen2 with PartitionFilterFunction\",\n            Code = \"let\n    Source = AzureStorage.DataLake(\"\"https://gbadls01.dfs.core.windows.net/public/powerbi_delta/FactInternetSales_part.delta\"\", [HierarchicalNavigation = false]),\n    DeltaTable = fn_ReadDeltaTable(Source, [PartitionFilterFunction=(x) => x[SalesTerritoryKey] = 3])\nin\n    DeltaTable\",\n            Result = \"#table( {\"\"ProductKey\"\", \"\"OrderDateKey\"\", \"\"SalesTerritoryKey\"\", \"\"Value\"\"}, { {\"\"A\"\", \"\"2020-01-01\"\", 3, 123} ,{\"\"B\"\", \"\"2020-04-02\"\", 3, 45} } )\"]\n        }\n    ]\n  \nin\n    Value.ReplaceType(fn_ReadDeltaTable, Value.ReplaceMetadata(Value.Type(fn_ReadDeltaTable), documentation));\r\nshared onelake = \"https://onelake.dfs.fabric.microsoft.com/64772dd4b90f/8a2c-4076c2573e09/Tables/\" meta [IsParameterQuery = true, IsParameterQueryRequired = true, Type = type text];\r\nshared nation = let\n    Source = fn_ReadDeltaTable(AzureStorage.DataLake(onelake &\"nation\"), [])\nin\n    Source;\r\n","connectionOverrides":[{"path":"https://onelake.dfs.fabric.microsoft.com/8b12-64772d/76c2573e09/Tables/nation","kind":"AzureDataLakeStorage","provider":"CdsA","authenticationKind":null,"environmentName":null,"apiName":null,"connectionName":"{\"kind\":\"AzureDataLakeStorage\",\"path\":\"https://onelake.dfs.fabric.microsoft.com/2dd4b90f/6c2573e09/Tables/nation\"}","audience":null}]},"annotations":[{"name":"pbi:QueryGroups","value":"[]"}],"entities":[{"$type":"LocalEntity","name":"nation","description":"","pbi:refreshPolicy":{"$type":"FullRefreshPolicy","location":"nation.csv"},"attributes":[{"name":"n_nationkey","dataType":"int64"},{"name":"n_name","dataType":"string"},{"name":"n_regionkey","dataType":"int64"},{"name":"n_comment","dataType":"string"}]}]}